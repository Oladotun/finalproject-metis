{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_dates(d1, d2):\n",
    "    date1 = datetime.datetime.strptime(d1.replace(\",\",\"\").strip(), '%Y-%m-%d')\n",
    "    date2 = datetime.datetime.strptime(d2.replace(\",\",\"\").strip(), '%Y-%m-%d')\n",
    "    return abs(date2-date1).days\n",
    "def timedatelinesplit(newKick, columnName):\n",
    "    newKick[columnName+\"_datetime\"] = newKick[columnName].apply(lambda x: datetime.datetime.fromtimestamp(float(x)).strftime('%Y-%m-%d , %H:%M:%S, %A'))\n",
    "    new = newKick[columnName+\"_datetime\"].str.split(\",\", n = 2, expand = True) \n",
    "    newKick[columnName+\"_date\"] = new[0]\n",
    "    newKick[columnName+\"_time\"] = new[1]\n",
    "    timeDayArray =  []\n",
    "    for hours in new[1]:\n",
    "        hours = hours.strip().split(\":\")\n",
    "\n",
    "        timeOfDay = \"\"\n",
    "        if int(hours[0]) in [12,13,14,15]:\n",
    "            timeOfDay = \"Afternoon\"\n",
    "            timeDayArray.append(timeOfDay)\n",
    "        elif hours[0] in [16,17,18,19,20]:\n",
    "            timeOfDay = \"Evening\"\n",
    "            timeDayArray.append(timeOfDay)\n",
    "        elif hours[0] in [20,21,22,23,24]:\n",
    "            timeOfDay = \"Night\"\n",
    "            timeDayArray.append(timeOfDay)\n",
    "        else:\n",
    "            timeOfDay = \"Morning\"\n",
    "            timeDayArray.append(timeOfDay)\n",
    "        \n",
    "    newKick[columnName+\"_moment\"] = timeDayArray\n",
    "    newKick[columnName+\"_day\"] = new[2]\n",
    "    \n",
    "    newKick = newKick.drop(columns = [columnName, columnName+\"_datetime\"])\n",
    "    \n",
    "    return newKick\n",
    "def getDfValues(df,names):\n",
    "    return df[names[0]], df[[names[1]]]\n",
    "def stateReturn(state):\n",
    "    if state == 'successful':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kick = pd.read_csv(\"Kickstarter001.csv\")\n",
    "# kick = pd.read_csv(\"/content/drive/My Drive/Kickstarter001.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop non needed columns\n",
    "newKick = kick.drop(columns = [\"is_backing\",\"is_starred\",\"static_usd_rate\",\"urls\",\n",
    "                     \"source_url\",\"currency_trailing_code\",\"id\",\"slug\", \"current_currency\",\n",
    "                     \"currency\",\"profile\",\"photo\",\"permissions\",\"created_at\",\n",
    "                     \"usd_pledged\",\"usd_type\",\"converted_pledged_amount\",\n",
    "                     \"currency_symbol\",\"disable_communication\",\"friends\",\n",
    "                     \"fx_rate\",\"is_starrable\"\n",
    "                     ])\n",
    "\n",
    "## Get only for USA\n",
    "newKick = newKick[newKick[\"country\"] == \"US\"]\n",
    "newKick = newKick.apply(lambda row: row[newKick['state'].isin(['successful','failed'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "namesArray = []\n",
    "slugArray = []\n",
    "categoryNameSlugArray = []\n",
    "for row in newKick[\"category\"]:\n",
    "    rowDict = ast.literal_eval(row)\n",
    "    namesArray.append(rowDict[\"name\"])\n",
    "    \n",
    "    slugArray.append(rowDict[\"slug\"].split(\"/\")[0])\n",
    "    categoryNameSlugArray.append(rowDict[\"name\"] + \" \"+rowDict[\"slug\"].split(\"/\")[0])\n",
    "newKick[\"category_name\"] = namesArray\n",
    "newKick[\"category_slug\"] = slugArray\n",
    "newKick[\"category_name_slug\"] = categoryNameSlugArray\n",
    "newKick = newKick.drop(columns = [\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "creatorArray = []\n",
    "creatorNameArray = []   \n",
    "for row in newKick[\"creator\"]:\n",
    "    rowDict = str(row)\n",
    "    rowArray = rowDict.split(\":\")\n",
    "    rowOneArray = rowArray[1].split(\",\")\n",
    "    creatorArray.append(rowOneArray[0])\n",
    "    rowTwoArray = rowArray[2].split(\",\")\n",
    "    creatorNameArray.append(rowTwoArray[0])\n",
    "newKick[\"creator_id\"] = creatorArray\n",
    "newKick[\"creator_name\"] = creatorNameArray\n",
    "\n",
    "newKick = newKick.drop(columns = [\"creator\"])\n",
    "\n",
    "locationArray = []\n",
    "locationNameArray = []  \n",
    "for row in newKick[\"location\"]:\n",
    "    if str(row) != 'nan': \n",
    "        rowDict = str(row)\n",
    "        rowArray = rowDict.split(\",\")\n",
    "        rowOneArray = rowArray[1].split(\":\")\n",
    "        rowTwoArray = rowArray[9].split(\":\")\n",
    "        \n",
    "        locationArray.append(rowOneArray[1].replace('\\\"',\"\").strip() + \" \"+ rowTwoArray[1].replace('\\\"',\"\").strip())\n",
    "    else:\n",
    "        locationArray.append(\"help\")\n",
    "\n",
    "newKick[\"city_state\"] = locationArray\n",
    "newKick = newKick.drop(columns = [\"location\", \"country\"])    \n",
    "newKick = timedatelinesplit(newKick, \"deadline\")\n",
    "newKick = timedatelinesplit(newKick, \"launched_at\")\n",
    "newKick[\"goal_pledged_diff\"] = newKick[\"pledged\"] - newKick[\"goal\"]\n",
    "newKick[\"duration_for_days\"] = newKick[[\"deadline_date\",\"launched_at_date\"]].apply(lambda x: diff_dates(x[0],x[1]), axis = 1)\n",
    "newKick[\"staff_pick\"] = newKick[\"staff_pick\"]\n",
    "\n",
    "\n",
    "### EDA GroupBys\n",
    "#category = newKick[newKick[\"state\"]==\"successful\"].groupby(\"category_slug\").count()\n",
    "category = newKick[newKick[\"state\"]==\"successful\"].groupby(\"category_slug\").count()\n",
    "cleanedUpKick = newKick.drop(columns=[\"backers_count\",\"pledged\",\"state_changed_at\",\n",
    "                      \"deadline_date\",\"deadline_time\",\"launched_at_date\", \"creator_name\" , \"category_name\",\"category_slug\",\"goal_pledged_diff\",\"spotlight\"])\n",
    "cols = ['goal', 'duration_for_days']\n",
    "subset_df = newKick[cols]\n",
    "\n",
    "#Standard Scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "scaled_df = ss.fit_transform(subset_df)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=cols)\n",
    "\n",
    "\n",
    "# Category Slug\n",
    "cleanedUpKick['staff_pick'] = cleanedUpKick['staff_pick'].astype(int)\n",
    "cleanedUpKick['state'] = cleanedUpKick['state'].apply(lambda x:stateReturn(x))\n",
    "cleanedWithKickTime = pd.get_dummies(cleanedUpKick, columns=[\"category_name_slug\",\"city_state\",\"launched_at_day\",\"launched_at_moment\"])\n",
    "cleanedWithKickTime[[\"goal\",\"duration_for_days\"]] = scaled_df\n",
    "cleanedWithKickTime.dropna(axis='rows',inplace=True)\n",
    "cleanedWithKickTimeBlurb = cleanedWithKickTime['blurb']\n",
    "cleanedWithKickTimeName =  cleanedWithKickTime['name']\n",
    "X = cleanedWithKickTime.drop(columns=[\"state\",\"blurb\",\"deadline_moment\",\"name\",\"deadline_day\",\"launched_at_time\",\"creator_id\"])\n",
    "Y = cleanedWithKickTime[\"state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7361963190184049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=101)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "result = model.fit(X_train, y_train)\n",
    "\n",
    "from sklearn import metrics\n",
    "prediction_test = model.predict(X_test)\n",
    "logistic_prediction_proba = model.predict_proba(X_test)[:,1]\n",
    "print (metrics.accuracy_score(y_test, prediction_test))\n",
    "\n",
    "## To get the weights of all the variables\n",
    "weights = pd.Series(model.coef_[0],index=X.columns.values)\n",
    "weightSorted = weights.sort_values(ascending = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV and LASSO Hyper Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7116564417177914\n",
      "0.7208588957055214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "model = LogisticRegression(penalty='l1') ## LASSO \n",
    "result = model.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = GridSearchCV(cv=None,\n",
    "             estimator=LogisticRegression(C=1.0, intercept_scaling=1,   \n",
    "               dual=False, fit_intercept=True, penalty='l1', tol=0.0001),\n",
    "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]})\n",
    "             \n",
    "resultclf = clf.fit(X_train, y_train)\n",
    "\n",
    "from sklearn import metrics\n",
    "prediction_test = model.predict(X_test)\n",
    "print (metrics.accuracy_score(y_test, prediction_test))\n",
    "prediction_test_clf = clf.predict(X_test)\n",
    "prediction_test_proba = clf.predict_proba(X_test)\n",
    "print (metrics.accuracy_score(y_test, prediction_test_clf))\n",
    "\n",
    "clf.feature_names = list(X_train.columns)\n",
    "# predictor/\n",
    "with open(\"logisticNeuralNet.pkl\", \"wb\") as f:\n",
    "    pickle.dump(clf, f)\n",
    "\n",
    "## To get the weights of all the variables\n",
    "weights = pd.Series(model.coef_[0],index=X.columns.values)\n",
    "weightSorted = weights.sort_values(ascending = False)\n",
    "weights_clf = pd.Series( clf.best_estimator_.coef_[0],index=X.columns.values)\n",
    "weightSorted_clf = weights_clf.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuned Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/opasina/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "weightSorted_clf.to_csv('finalPages.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city_state_Hilton Head Island SC         7.110524\n",
      "city_state_Grand Rapids MI               6.045215\n",
      "category_name_slug_Fiction publishing    5.972428\n",
      "city_state_Fort Davis TX                 5.903726\n",
      "city_state_Lithonia GA                   5.669309\n",
      "city_state_Hinsdale IL                   5.586010\n",
      "city_state_Princeton NJ                  5.543553\n",
      "category_name_slug_Video Games games     5.528623\n",
      "city_state_Dartmouth MA                  5.491518\n",
      "category_name_slug_Crafts crafts         5.482881\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(weightSorted_clf.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_name_slug_Comedy music            -4.647498\n",
      "city_state_Yoakum TX                       -4.887002\n",
      "category_name_slug_Childrenswear fashion   -4.919344\n",
      "city_state_Hilo HI                         -4.959460\n",
      "city_state_Tulum Quintana Roo              -4.989510\n",
      "category_name_slug_Live Games games        -5.777723\n",
      "category_name_slug_Farms food              -5.970017\n",
      "category_name_slug_Food Trucks food        -6.430207\n",
      "category_name_slug_Web journalism          -7.200177\n",
      "category_name_slug_Mobile Games games      -7.348971\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(weightSorted_clf.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Dense, Embedding, Reshape, Activation, \n",
    "                          SimpleRNN, LSTM, Convolution1D, \n",
    "                          MaxPooling1D, Dropout, Bidirectional,SpatialDropout1D)\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK LSTM METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "def create_glove_embedded():\n",
    "    embeddings_index = {}\n",
    "    GLOVE = '/content/drive/My Drive/glove.6B.100d.txt'\n",
    "    f = open(GLOVE)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedded_matrix(embeddings_index,word_index,EMBEDDING_DIM):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer_word_index(cleanedWithKickTimeBlurb):\n",
    "    tokenizer = Tokenizer(num_words = len(cleanedWithKickTimeBlurb), filters=\"\"\"!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\"\"\")\n",
    "    tokenizer.fit_on_texts(cleanedWithKickTimeBlurb.values)\n",
    "    word_index = tokenizer.word_index\n",
    "    return word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dill\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/7a/70803635c850e351257029089d38748516a280864c97cbc73087afef6d51/dill-0.3.0.tar.gz (151kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 5.1MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: dill\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/opasina/Library/Caches/pip/wheels/c9/de/a4/a91eec4eea652104d8c81b633f32ead5eb57d1b294eab24167\n",
      "Successfully built dill\n",
      "Installing collected packages: dill\n",
      "Successfully installed dill-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import pickle\n",
    "def convertTextToWord(word):\n",
    "    tokenizer = Tokenizer(num_words = len(word.split()), filters=\"\"\"!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\"\"\")\n",
    "    tokenizer.fit_on_texts(word)\n",
    "    word_index = tokenizer.word_index\n",
    "    X_blurb = tokenizer.texts_to_sequences(word)\n",
    "    X_blurb = pad_sequences(X_blurb, maxlen=maxlen)\n",
    "    \n",
    "    return X_blurb\n",
    "# ser = dill.dumps(convertTextToWord)\n",
    "\n",
    "#Pickle model\n",
    "with open(\"converTextToWord.pkl\", \"wb\") as f:\n",
    "    pickle.dump(convertTextToWord, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuralNetworkForText(cleanedWithKickTimeBlurb,embedding_matrix,maxlen,lstm_param):\n",
    "    tokenizer = Tokenizer(num_words = len(cleanedWithKickTimeBlurb), filters=\"\"\"!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\"\"\")\n",
    "    tokenizer.fit_on_texts(cleanedWithKickTimeBlurb.values)\n",
    "    word_index = tokenizer.word_index\n",
    "    X_blurb = tokenizer.texts_to_sequences(cleanedWithKickTimeBlurb)\n",
    "    X_blurb = pad_sequences(X_blurb, maxlen=maxlen)\n",
    "    X_train_blurb, X_test_blurb, y_train_blurb, y_test_blurb = train_test_split(X_blurb,Y, test_size = 0.20)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                              EMBEDDING_DIM,weights=[embedding_matrix],\n",
    "                              trainable=False))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(LSTM(lstm_param, dropout=0.2, recurrent_dropout=0.2))\n",
    "    # model.add(Dropout(0.20))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                    optimizer='adam',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    epochs = 20\n",
    "    batch_size = 32\n",
    "    model.fit(X_train_blurb, y_train_blurb, batch_size=batch_size, epochs=epochs, \n",
    "                validation_data=(X_test_blurb, y_test_blurb),\n",
    "           callbacks=[EarlyStopping(patience=10, verbose=1,restore_best_weights=True),\n",
    "                      ReduceLROnPlateau(factor=.5, patience=3, verbose=1)])\n",
    "    hidden_features_blurb = model.predict(X_test_blurb)\n",
    "    \n",
    "    hidden_features_result = model.predict_proba(X_test_blurb)\n",
    "    \n",
    "    hidden_features_array = [x[0] for x in hidden_features_result]\n",
    "\n",
    "\n",
    "    return model, hidden_features_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK KICKSTARTER BLURB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = create_glove_embedded()\n",
    "word_index = get_tokenizer_word_index(cleanedWithKickTimeBlurb)\n",
    "embedding_matrix = embedded_matrix(embeddings_index,word_index,EMBEDDING_DIM)\n",
    "predictblurbModelProb = neuralNetworkForText(cleanedWithKickTimeBlurb,embedding_matrix,maxlen,50)\n",
    "predictblurbModel = predictblurbModelProb[0]\n",
    "predictblurbProb = predictblurbModelProb[1]\n",
    "\n",
    "#Pickle model\n",
    "with open(\"blurbNeuralNet.pkl\", \"wb\") as f:\n",
    "    pickle.dump(predictblurbModel, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK KICKSTARTER NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_name = get_tokenizer_word_index(cleanedWithKickTimeName)\n",
    "embedding_matrix_name = embedded_matrix(embeddings_index,word_index_name,EMBEDDING_DIM)\n",
    "predictNameModelProb = neuralNetworkForText(cleanedWithKickTimeName,embedding_matrix_name,maxlen,40)\n",
    "predictNameModel = predictNameModelProb[0]\n",
    "predictNameProb = predictNameModelProb[1]\n",
    "\n",
    "#Pickle model\n",
    "with open(\"nameNeuralNet.pkl\", \"wb\") as f:\n",
    "    pickle.dump(predictNameModel, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING SINGLE PREDICTION FROM MULTIPLE PREDICTIONS (ENSEMBLING- STACKING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probToResult = pd.DataFrame(\n",
    "    {'predict_name': predictNameProb,\n",
    "     'predict_blurb': predictblurbProb,\n",
    "     'logistic_prediction' : prediction_test_proba[:,1],\n",
    "     'result': y_test\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = ['predict_name', 'predict_blurb','logistic_prediction']\n",
    "# subset_df = probToResult[cols]\n",
    "# #Standard Scaler\n",
    "# ss = StandardScaler()\n",
    "# scaled_df_prob = ss.fit_transform(subset_df)\n",
    "# scaled_df_prob = pd.DataFrame(scaled_df_prob, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaled_df_prob[['predict_name','predict_blurb','logistic_prediction']]\n",
    "Y = probToResult['result']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=101)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "result = model.fit(X_train, y_train)\n",
    "\n",
    "# save the model to disk\n",
    "with open(\"lrResult.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
